{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9f96b6-f713-43fe-a543-164a5372f1c5",
   "metadata": {},
   "source": [
    "# Recommending Movies\n",
    "\n",
    "In this track you will be introduced to the MovieLens dataset. You will walk through how to assess it's use for ALS, build out a full cross-validated ALS model on it, and learn how to evaluate it's performance. This will be the foundation for all subsequent ALS models you build using Pyspark.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21458f8-1acd-43a2-9cc6-306b920957ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from typing import List, Tuple\n",
    "from pprint import pprint\n",
    "from environment import SEED\n",
    "from sklearn.decomposition import NMF\n",
    "from pyspark.sql import SparkSession, Row, DataFrame as SparkDataframe, functions as F\n",
    "from pyspark.sql.types import (StructType, StructField,\n",
    "                               DoubleType, IntegerType, StringType, TimestampType)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb12e47-14ed-4e04-bc56-4e38f659c2b3",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067fae1d-87c8-420a-9970-2287108b60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .master('local[*]') \\\n",
    "                     .appName('spark_application') \\\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", True)  # eval DataFrame in notebooks\n",
    "                     .config(\"spark.driver.memory\", \"10g\")\n",
    "                     .config(\"spark.driver.maxResultSize\", \"10G\")\n",
    "                     .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty('spark.executor.memory', '10G')\n",
    "sc.setCheckpointDir(\"ml-checkpoint/\")\n",
    "\n",
    "print(f'Spark version: {spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a725dc5b-c397-402c-aaed-0159c162353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current driver memory: 10g\n",
      "Current number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Review current configuration\n",
    "print(f\"Current driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Current number of partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea8ed5-a73b-4a7f-9587-9dff5833f1d9",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Source of datasets: [Kaggle](https://www.kaggle.com/c/msdchallenge/data)\n",
    "\n",
    "Token to validate access can be generated following the instructions from [here](https://github.com/Kaggle/kaggle-api).\n",
    "```\n",
    "pip install kaggle\n",
    "kaggle competitions download -c msdchallenge\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d70cb-8219-4227-9016-4fa59f0b7e25",
   "metadata": {},
   "source": [
    "### Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc1cfb4-8894-421a-a5c4-7eedcda4f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (386213, 2)\n",
      "root\n",
      " |-- songCode: string (nullable = true)\n",
      " |-- songId: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>songCode</th><th>songId</th></tr>\n",
       "<tr><td>SOAAADD12AB018A9DD</td><td>1</td></tr>\n",
       "<tr><td>SOAAADE12A6D4F80CC</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------+------+\n",
       "|          songCode|songId|\n",
       "+------------------+------+\n",
       "|SOAAADD12AB018A9DD|     1|\n",
       "|SOAAADE12A6D4F80CC|     2|\n",
       "+------------------+------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_songs = StructType([\n",
    "    StructField(\"songCode\", StringType()),\n",
    "    StructField(\"songId\", IntegerType())\n",
    "])\n",
    "songs_data = spark.read.csv('data-sources/songs/songs.txt', header=False, sep=' ', schema=schema_songs)\n",
    "\n",
    "# Reviewing the result\n",
    "songs_data.createOrReplaceTempView(\"songs\")\n",
    "print(f'Dataframe shape: ({songs_data.count()}, {len(songs_data.columns)})')\n",
    "songs_data.printSchema()\n",
    "songs_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782856c3-0159-410b-bd59-d2e1c4e69f88",
   "metadata": {},
   "source": [
    "### Disks (Song Details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13486ab-939d-47ae-8713-634778261bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (999056, 4)\n",
      "root\n",
      " |-- songCode: string (nullable = true)\n",
      " |-- singer: string (nullable = true)\n",
      " |-- songName: string (nullable = true)\n",
      " |-- songId: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>songCode</th><th>singer</th><th>songName</th><th>songId</th></tr>\n",
       "<tr><td>SOAAAKE12A67AD7460</td><td>Les Compagnons De...</td><td>Le Galrien</td><td>NULL</td></tr>\n",
       "<tr><td>SOAABAM12AB0180E94</td><td>NIGHT FLIGHT</td><td>Little Things</td><td>NULL</td></tr>\n",
       "<tr><td>SOAABGK12AB0185F7F</td><td>Les Trompettes Du...</td><td>La retraite de pi...</td><td>NULL</td></tr>\n",
       "<tr><td>SOAABLP12AB017D3FF</td><td>Ojm</td><td>The Sleeper</td><td>NULL</td></tr>\n",
       "<tr><td>SOAABSB12A8C143E55</td><td>UK Subs</td><td>Organised  Crime</td><td>37</td></tr>\n",
       "<tr><td>SOAACGG12A58A7A034</td><td>Matt Costa</td><td>Ballad Of Miss Kate</td><td>48</td></tr>\n",
       "<tr><td>SOAACJX12AB018B34A</td><td>Norrisman</td><td>Welcome To Say</td><td>NULL</td></tr>\n",
       "<tr><td>SOAACUH12A8AE48B92</td><td>Infinite Mass</td><td>I Dont Care</td><td>NULL</td></tr>\n",
       "<tr><td>SOAACVP12A8C134567</td><td>Missing Persons</td><td>Rock And Roll Sus...</td><td>61</td></tr>\n",
       "<tr><td>SOAADAS12A58A784EC</td><td>Fort Knox Five fe...</td><td>How to Start a Band</td><td>70</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------+--------------------+--------------------+------+\n",
       "|          songCode|              singer|            songName|songId|\n",
       "+------------------+--------------------+--------------------+------+\n",
       "|SOAAAKE12A67AD7460|Les Compagnons De...|          Le Galrien|  NULL|\n",
       "|SOAABAM12AB0180E94|        NIGHT FLIGHT|       Little Things|  NULL|\n",
       "|SOAABGK12AB0185F7F|Les Trompettes Du...|La retraite de pi...|  NULL|\n",
       "|SOAABLP12AB017D3FF|                 Ojm|         The Sleeper|  NULL|\n",
       "|SOAABSB12A8C143E55|             UK Subs|    Organised  Crime|    37|\n",
       "|SOAACGG12A58A7A034|          Matt Costa| Ballad Of Miss Kate|    48|\n",
       "|SOAACJX12AB018B34A|           Norrisman|      Welcome To Say|  NULL|\n",
       "|SOAACUH12A8AE48B92|       Infinite Mass|         I Dont Care|  NULL|\n",
       "|SOAACVP12A8C134567|     Missing Persons|Rock And Roll Sus...|    61|\n",
       "|SOAADAS12A58A784EC|Fort Knox Five fe...| How to Start a Band|    70|\n",
       "+------------------+--------------------+--------------------+------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_disks = StructType([\n",
    "    StructField(\"soundtrack\", StringType()),\n",
    "    StructField(\"songCode\", StringType()),\n",
    "    StructField(\"singer\", StringType()),\n",
    "    StructField(\"songName\", StringType())\n",
    "])\n",
    "disks_data = spark.read.csv('data-sources/songs/disks.txt', header=False, sep='<SEP>', schema=schema_disks)\n",
    "\n",
    "# Cleaning and mutating/adding some colums\n",
    "disks_data = disks_data.withColumn(\"songName\",    # Removing special characters\n",
    "                                   F.regexp_replace(F.col(\"songName\"), \"[^A-Za-z0-9\\s]\", \"\"))\n",
    "disks_data = (disks_data.groupby('songCode').agg(F.last('singer').alias('singer'),   # Making unique records\n",
    "                                                 F.last('songName').alias('songName')))\n",
    "disks_data = disks_data.join(songs_data, 'songCode', 'left')  # Adding the Id to songs\n",
    "\n",
    "# Reviewing the result\n",
    "disks_data.createOrReplaceTempView(\"disks\")\n",
    "print(f'Dataframe shape: ({disks_data.count()}, {len(disks_data.columns)})')\n",
    "disks_data.printSchema()\n",
    "disks_data.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329025e6-bb06-44c7-b620-2a2afe8e2699",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8ce3bb-a540-4203-82ee-ff55265a5b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (110000, 2)\n",
      "root\n",
      " |-- userCode: string (nullable = true)\n",
      " |-- userId: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>userCode</th><th>userId</th></tr>\n",
       "<tr><td>fd50c4007b68a3737...</td><td>0</td></tr>\n",
       "<tr><td>d7083f5e1d50c2642...</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------+\n",
       "|            userCode|userId|\n",
       "+--------------------+------+\n",
       "|fd50c4007b68a3737...|     0|\n",
       "|d7083f5e1d50c2642...|     1|\n",
       "+--------------------+------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_users = StructType([\n",
    "    StructField(\"userCode\", StringType())\n",
    "])\n",
    "users_data = spark.read.csv('data-sources/songs/users.txt', header=False, sep=' ', schema=schema_users)\n",
    "\n",
    "# Mutating columns\n",
    "users_data = users_data.coalesce(1).withColumn('userId', F.monotonically_increasing_id()).persist()\n",
    "\n",
    "# Reviewing the result\n",
    "users_data.createOrReplaceTempView(\"users\")\n",
    "print(f'Dataframe shape: ({users_data.count()}, {len(users_data.columns)})')\n",
    "users_data.printSchema()\n",
    "users_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9aceea-f916-4361-8da4-5181bc453ef3",
   "metadata": {},
   "source": [
    "### Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2a2493-6386-4a29-a9f2-6e2a5b29efe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (2390, 5)\n",
      "root\n",
      " |-- userCode: string (nullable = true)\n",
      " |-- songCode: string (nullable = true)\n",
      " |-- numPlays: integer (nullable = true)\n",
      " |-- songId: integer (nullable = true)\n",
      " |-- userId: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>userCode</th><th>songCode</th><th>numPlays</th><th>songId</th><th>userId</th></tr>\n",
       "<tr><td>f5ecd768453281573...</td><td>SOEPZQS12A8C1436C7</td><td>16</td><td>74321</td><td>110</td></tr>\n",
       "<tr><td>f5ecd768453281573...</td><td>SOFRQTD12A81C233C0</td><td>14</td><td>91177</td><td>110</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------------+--------+------+------+\n",
       "|            userCode|          songCode|numPlays|songId|userId|\n",
       "+--------------------+------------------+--------+------+------+\n",
       "|f5ecd768453281573...|SOEPZQS12A8C1436C7|      16| 74321|   110|\n",
       "|f5ecd768453281573...|SOFRQTD12A81C233C0|      14| 91177|   110|\n",
       "+--------------------+------------------+--------+------+------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_ratings = StructType([\n",
    "    StructField(\"userCode\", StringType()),\n",
    "    StructField(\"songCode\", StringType()),\n",
    "    StructField(\"numPlays\", IntegerType())\n",
    "])\n",
    "ratings_data = spark.read.csv('data-sources/songs/ratings.txt', \n",
    "                            header=False, sep='\\t', schema=schema_ratings)\n",
    "\n",
    "# Adding Id's to Users and Songs\n",
    "ratings_data = (ratings_data.join(songs_data, 'songCode', 'left')\n",
    "                            .join(users_data, 'userCode', 'left')\n",
    "                            .dropna())\n",
    "\n",
    "# Taking just part of the data\n",
    "ratings_data = (ratings_data.filter(F.col('userId').isin(ratings_data.groupby('userId').count()\n",
    "                                                                     .sort('count', ascending=False)\n",
    "                                                                     .limit(321).select('userId')\n",
    "                                                                     .toPandas().userId.to_list()))\n",
    "                            .filter(F.col('songId').isin(ratings_data.groupby('songId').count()\n",
    "                                                                     .sort('count', ascending=False)\n",
    "                                                                     .limit(729).select('songId')\n",
    "                                                                     .toPandas().songId.to_list())))\n",
    "\n",
    "# Reviewing the result\n",
    "ratings_data.createOrReplaceTempView(\"ratings\")\n",
    "print(f'Dataframe shape: ({ratings_data.count()}, {len(ratings_data.columns)})')\n",
    "ratings_data.printSchema()\n",
    "ratings_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3784f5-bc8c-416d-935a-7dce6b074c26",
   "metadata": {},
   "source": [
    "### Movie Binary Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07dad05-2e02-43b7-ad9f-a7dd62da9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 1995-01-09 05:46:44 - 2023-10-12 20:29:07\n",
      "Dataframe shape: (36471, 4)\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>userId</th><th>movieId</th><th>rating</th><th>timestamp</th></tr>\n",
       "<tr><td>178</td><td>223</td><td>1</td><td>2023-10-04 18:28:39</td></tr>\n",
       "<tr><td>178</td><td>260</td><td>1</td><td>2023-10-04 18:36:52</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+-------+------+-------------------+\n",
       "|userId|movieId|rating|          timestamp|\n",
       "+------+-------+------+-------------------+\n",
       "|   178|    223|     1|2023-10-04 18:28:39|\n",
       "|   178|    260|     1|2023-10-04 18:36:52|\n",
       "+------+-------+------+-------------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file\n",
    "schema_bin_movies = StructType([\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"movieId\", IntegerType()),\n",
    "    StructField(\"rating\", DoubleType()),\n",
    "    StructField(\"timestamp\", IntegerType())\n",
    "])\n",
    "binary_movies_data = spark.read.csv('data-sources/movies/ratings.csv', header=True, schema=schema_bin_movies)\n",
    "\n",
    "# Cleaning and mutating some columns\n",
    "binary_movies_data = binary_movies_data.withColumn('timestamp', F.to_timestamp(F.from_unixtime('timestamp')))\n",
    "date_range = binary_movies_data.select('timestamp').agg(F.min('timestamp'), F.max('timestamp')).collect()[0]\n",
    "print(f\"Date range: {date_range[0]} - {date_range[1]}\")\n",
    "\n",
    "# Taking just part of the data\n",
    "binary_movies_data = (binary_movies_data.where('timestamp >= \"2023-09-12\"')\n",
    "                            .withColumn('rating', F.lit(1)))\n",
    "# binary_movies_data = binary_movies_data.sample(withReplacement=False, fraction=.05, seed=SEED)\n",
    "binary_movies_data = (binary_movies_data.filter(F.col('userId')\n",
    "                                                 .isin(binary_movies_data.groupby('userId').count()\n",
    "                                                 .sort('count', ascending=False)\n",
    "                                                 .limit(321).select('userId')\n",
    "                                                 .toPandas().userId.to_list()))\n",
    "                                        .filter(F.col('movieId')\n",
    "                                                 .isin(binary_movies_data.groupby('movieId').count()\n",
    "                                                 .sort('count', ascending=False)\n",
    "                                                 .limit(729).select('movieId')\n",
    "                                                 .toPandas().movieId.to_list())))\n",
    "\n",
    "# Reviewing the result\n",
    "binary_movies_data.createOrReplaceTempView(\"ratings\")\n",
    "print(f'Dataframe shape: ({binary_movies_data.count()}, {len(binary_movies_data.columns)})')\n",
    "binary_movies_data.printSchema()\n",
    "binary_movies_data.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79709f1-ff5e-4b2b-8168-a273730bb43f",
   "metadata": {},
   "source": [
    "### Tables catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544d8bd5-3e6f-4f7c-9f51-c4cee7a56df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='disks', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='ratings', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='songs', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='users', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205476d-ee4d-4bde-9814-da069cbbc0da",
   "metadata": {},
   "source": [
    "## Util functions\n",
    "\n",
    "### add_zeros_to_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b147f0f1-4aa4-4665-bba4-2990e2e7e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros_to_ratings(df, userCol=\"userId\", productCol=\"songId\"):\n",
    "    # Extracts distinct users\n",
    "    users = df.select(userCol).distinct()\n",
    "    print(f'{users.count()} unique users.')\n",
    "    \n",
    "    # Extracts distinct products\n",
    "    products = df.select(productCol).distinct()\n",
    "    print(f'{products.count()} unique products.')\n",
    "    \n",
    "    # Joins users and products, fills blanks with 0\n",
    "    cross_join = (users.crossJoin(products)\n",
    "                       .join(df, [userCol, productCol], \"left\")\n",
    "                       .fillna(0))\n",
    "    print(f'{products.count()*users.count()} total expected records.')\n",
    "    \n",
    "    return cross_join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cf604-25e8-42ff-992c-44a6099b0c27",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "## Introduction to the Million Songs Dataset\n",
    "\n",
    "### Preparing ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d1389f-aa45-4364-8207-ba65fd4e474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 unique users.\n",
      "643 unique products.\n",
      "190328 total expected records.\n",
      "+------+------+--------+\n",
      "|userId|songId|numPlays|\n",
      "+------+------+--------+\n",
      "|94259 |25150 |97      |\n",
      "|64180 |319911|90      |\n",
      "|4770  |301674|72      |\n",
      "+------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "Total rows = 190328\n",
      "\n",
      "Consumed time: 0.13290206591288248 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = ratings_data.drop(*['userCode', 'songCode'])\n",
    "df_zeros = add_zeros_to_ratings(df)\n",
    "df_zeros.sort('numPlays', ascending=False).show(3, truncate=False)\n",
    "print(f'''\n",
    "Total rows = {df_zeros.count()}\n",
    "\n",
    "Consumed time: {(time.time() - start)/60} min\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf7537-270e-4734-9e83-e59afd599186",
   "metadata": {},
   "source": [
    "### MovieLens sparsity\n",
    "\n",
    "To measure how sparse is the data.\n",
    "$$ Sparcity = 1 - \\frac{Number Of Ratings In Matrix}{Number Of Users \\times Number Of Songs} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d68f08-e074-469f-b3f9-d9f9b2878c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Million Songs Dataset Sparsity: 0.9874427304442857\n"
     ]
    }
   ],
   "source": [
    "# Calculating MovieLens sparsity\n",
    "number_of_ratings = df.count()\n",
    "number_of_users = df.select('userId').distinct().count()\n",
    "number_of_movies = df.select('songId').distinct().count()\n",
    "\n",
    "sparsity = 1 - (number_of_ratings / (number_of_users * number_of_movies))\n",
    "print('Million Songs Dataset Sparsity:', sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba5a7b-1031-49ad-b4d8-7106f6a7cb28",
   "metadata": {},
   "source": [
    "## Ex. 1 - Grouped summary statistics\n",
    "\n",
    "In this exercise, we are going to combine the `.groupBy()` and `.filter()` methods that you've used previously to calculate the `min()` and `avg()` number of users that have rated each song, and the `min()` and `avg()` number of songs that each user has rated.\n",
    "\n",
    "Because our data now includes 0's for items not yet consumed, we'll need to `.filter()` them out when doing grouped summary statistics like this. The msd dataset is provided for you here. The `col()`, `min()`, and `avg()` functions from `pyspark.sql.functions` have been imported for you.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. As an example, the `.filter()`, `.groupBy()` and `.count()` methods are applied to the msd dataset along with `.select()` and `min()` to return the smallest number of ratings that any song in the dataset has received. Use this as a model to calculate the `avg()` number of implicit ratings the songs in msd have received.\n",
    "2. Using the same model, find the `min()` and `avg()` number of implicit ratings that userIds have provided in the msd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ef6f73-31d3-44a0-b055-4f1a3031f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum implicit ratings for a song: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Average implicit ratings per song: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|3.7169517884914463|\n",
      "+------------------+\n",
      "\n",
      "Minimum implicit ratings from a user: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Average implicit ratings per user: \n",
      "+-----------------+\n",
      "|       avg(count)|\n",
      "+-----------------+\n",
      "|8.074324324324325|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "df_zeros.filter(F.col(\"numPlays\") > 0).groupBy(\"songId\").count().select(F.min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "df_zeros.filter(F.col(\"numPlays\") > 0).groupBy(\"songId\").count().select(F.avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "df_zeros.filter(F.col(\"numPlays\") > 0).groupBy(\"userId\").count().select(F.min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings for users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "df_zeros.filter(F.col(\"numPlays\") > 0).groupBy(\"userId\").count().select(F.avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be6e5b5-906e-4ede-b7d2-b900359db30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum implicit ratings for a song: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Average implicit ratings per song: \n",
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|3.7169517884914463|\n",
      "+------------------+\n",
      "\n",
      "Minimum implicit ratings from a user: \n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "Average implicit ratings per user: \n",
      "+-----------------+\n",
      "|       avg(count)|\n",
      "+-----------------+\n",
      "|8.074324324324325|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "df.groupBy(\"songId\").count().select(F.min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "df.groupBy(\"songId\").count().select(F.avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "df.groupBy(\"userId\").count().select(F.min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings for users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "df.groupBy(\"userId\").count().select(F.avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99bb5b-cca7-4c3a-a6b6-1a6a0cc127be",
   "metadata": {},
   "source": [
    "## Ex. 2 - Add zeros\n",
    "\n",
    "Many recommendation engines use implicit ratings. In many cases these datasets don't include behavior counts for items that a user has never purchased. In these cases, you'll need to add them and include zeros. The dataframe Z is provided for you. It contains userId's, productId's and num_purchases which is the number of times a user has purchased a specific product.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Take a look at the dataframe `Z` using the `.show()` method.\n",
    "2. Extract the distinct `userId`s and `productId`s from `Z` using the `.distinct()` method. Call the results `users` and `products` respectively.\n",
    "3. Perform a `.crossJoin()` on the `users` and `products` dataframes. Call the result `cj`.\n",
    "4. `\"left\"` join `cj` to the original ratings dataframe `Z` on `[\"userId\", \"productId\"]`. Call the `.fillna(0)` method on the result to fill in the blanks with zeros. Call the result `Z_expanded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b99371e0-0047-4cee-8019-d53e86c61eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------+\n",
      "|userId|productId|num_purchases|\n",
      "+------+---------+-------------+\n",
      "|  2112|      777|            1|\n",
      "|     7|       44|           23|\n",
      "|  1132|      227|            9|\n",
      "+------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "Z = spark.createDataFrame([Row(userId=2112, productId=777, num_purchases=1),\n",
    "                           Row(userId=7, productId=44, num_purchases=23),\n",
    "                           Row(userId=1132, productId=227, num_purchases=9),\n",
    "                           Row(userId=686, productId=1981, num_purchases=2),\n",
    "                           Row(userId=42, productId=2390, num_purchases=5),\n",
    "                           Row(userId=13, productId=1662, num_purchases=21),\n",
    "                           Row(userId=2112, productId=1492, num_purchases=8),\n",
    "                           Row(userId=22, productId=1811, num_purchases=96)])\n",
    "\n",
    "# View the data\n",
    "Z.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401328b3-a35d-4bde-8427-6af504a1e55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 unique users.\n",
      "8 unique products.\n",
      "56 total expected records.\n",
      "Total rows = 56\n",
      "+------+---------+-------------+\n",
      "|userId|productId|num_purchases|\n",
      "+------+---------+-------------+\n",
      "|22    |1811     |96           |\n",
      "|7     |44       |23           |\n",
      "|13    |1662     |21           |\n",
      "+------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z_expanded = add_zeros_to_ratings(Z, userCol=\"userId\", productCol=\"productId\")\n",
    "print(f'Total rows = {Z_expanded.count()}')\n",
    "Z_expanded.sort('num_purchases', ascending=False).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fff79e-99c4-4688-8a57-4054d4684397",
   "metadata": {},
   "source": [
    "# Implicit Ratings using user behavior counts\n",
    "\n",
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "657eb775-c253-49e5-8fed-6fcce193dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+\n",
      "|userId|songId|numPlays|\n",
      "+------+------+--------+\n",
      "|  3702|170306|       1|\n",
      "|  4748|123630|       3|\n",
      "+------+------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Training set: 1912, Testing set: 478\n",
      "+------+------+--------+------------+\n",
      "|userId|songId|numPlays|prediction  |\n",
      "+------+------+--------+------------+\n",
      "|110   |189050|18      |0.022847397 |\n",
      "|1115  |112304|2       |0.0         |\n",
      "|1198  |55106 |2       |0.012004802 |\n",
      "|3040  |289658|2       |0.27410442  |\n",
      "|3702  |349271|1       |0.0069155516|\n",
      "|4608  |54368 |1       |0.0         |\n",
      "|4770  |52176 |2       |1.4001423   |\n",
      "|5401  |217471|8       |0.60500014  |\n",
      "|6401  |91773 |1       |0.004497187 |\n",
      "|6401  |329834|1       |0.22024302  |\n",
      "|7494  |51296 |1       |0.0         |\n",
      "|9641  |353640|1       |0.031095814 |\n",
      "|14687 |25323 |16      |1.0028197   |\n",
      "|14687 |353700|1       |0.58127636  |\n",
      "|19497 |207916|1       |0.12981367  |\n",
      "+------+------+--------+------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "Min predidction: 0.0, Max prediction: 1.4001423120498657, Threshold: 0.5\n",
      "+------+------+--------+------------+-----------+\n",
      "|userId|songId|numPlays|prediction  |Recommended|\n",
      "+------+------+--------+------------+-----------+\n",
      "|110   |189050|18      |0.022847397 |No         |\n",
      "|1115  |112304|2       |0.0         |No         |\n",
      "|1198  |55106 |2       |0.012004802 |No         |\n",
      "|3040  |289658|2       |0.27410442  |No         |\n",
      "|3702  |349271|1       |0.0069155516|No         |\n",
      "|4608  |54368 |1       |0.0         |No         |\n",
      "|4770  |52176 |2       |1.4001423   |Yes        |\n",
      "|5401  |217471|8       |0.60500014  |Yes        |\n",
      "|6401  |91773 |1       |0.004497187 |No         |\n",
      "|6401  |329834|1       |0.22024302  |No         |\n",
      "|7494  |51296 |1       |0.0         |No         |\n",
      "|9641  |353640|1       |0.031095814 |No         |\n",
      "|14687 |25323 |16      |1.0028197   |Yes        |\n",
      "|14687 |353700|1       |0.58127636  |Yes        |\n",
      "|19497 |207916|1       |0.12981367  |No         |\n",
      "+------+------+--------+------------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = ratings_data.select('userId', 'songId', 'numPlays').repartition(5)\n",
    "df.show(2)\n",
    "\n",
    "# Split into train and test set.\n",
    "df_train, df_test = df.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Training set: {df_train.count()}, Testing set: {df_test.count()}\")\n",
    "\n",
    "# Build ALS model\n",
    "als_model = ALS(userCol=\"userId\", itemCol=\"songId\", ratingCol=\"numPlays\",\n",
    "                rank=10, maxIter=10, regParam=.1, alpha=10,\n",
    "                nonnegative=True, coldStartStrategy=\"drop\", implicitPrefs=True, seed=SEED).fit(df_train)\n",
    "\n",
    "# Make some predictions\n",
    "prediction = als_model.transform(df_test)\n",
    "prediction.show(15, truncate=False)\n",
    "pred_range = prediction.agg(F.min('prediction').alias('min'), \n",
    "                            F.max('prediction').alias('max')).collect()[0].asDict()\n",
    "threshold = .5\n",
    "print(f\"Min predidction: {pred_range['min']}, Max prediction: {pred_range['max']}, Threshold: {threshold}\")\n",
    "\n",
    "# Setting a threshold to interpret the prediction\n",
    "prediction = prediction.withColumn('Recommended', \n",
    "                                   F.when(F.col('prediction')>threshold, 'Yes').otherwise('No'))\n",
    "prediction.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ea403-a646-472a-a44a-14b5b6b2ab4d",
   "metadata": {},
   "source": [
    "## With ROEM (Rank Ordering Error Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acfc9c07-8c62-4ce9-95be-c1a8d705859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROEM_cv(ratings_df: SparkDataframe, \n",
    "            userCol: str=\"userId\", itemCol: str=\"songId\", ratingCol: str=\"numPlays\",\n",
    "            ranks: List=[10, 50, 100, 150, 200],\n",
    "            maxIters: List=[10, 25, 50, 100, 200, 400],\n",
    "            regParams: List=[.05, .1, .15],\n",
    "            alphas: List=[10, 40, 80, 100],\n",
    "            seed=0) -> Tuple[ALSModel, SparkDataframe]:\n",
    "    '''\n",
    "    This function is an Alternate cross validation approach for ALS models with implicit ratings \n",
    "    utilizing an expected percent ranking metric for model performance evaluation.\n",
    "    The ratings df should contain all possible combinations between users and items, with rating = 0\n",
    "    for those cases where there is no registered rating.\n",
    "    Source: https://github.com/jamenlong/ALS_expected_percent_rank_cv/blob/master/ROEM_cv.py\n",
    "    By [jamenlong](https://github.com/jamenlong)\n",
    "    '''\n",
    "    print(f'''\n",
    "    Total models to create: {len(ranks) * len(maxIters) * len(regParams) * len(alphas)}\n",
    "    ''')\n",
    "    ratings_df = ratings_df.orderBy(F.rand())  # Shuffling to ensure randomness\n",
    "\n",
    "    # Building train and validation test sets\n",
    "    train, validate = ratings_df.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "    # Building 5 folds within the training set.\n",
    "    test1, test2, test3, test4, test5 = train.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed=seed)\n",
    "    train1 = test2.union(test3).union(test4).union(test5)\n",
    "    train2 = test3.union(test4).union(test5).union(test1)\n",
    "    train3 = test4.union(test5).union(test1).union(test2)\n",
    "    train4 = test5.union(test1).union(test2).union(test3)\n",
    "    train5 = test1.union(test2).union(test3).union(test4)\n",
    "\n",
    "    # Creating variables that will be replaced by the best model's hyperparameters for subsequent printing\n",
    "    best_validation_roem = 9999999999999\n",
    "    best_rank, best_maxIter, best_regParam, best_alpha, best_model, best_predictions = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Looping through each combindation of hyperparameters to ensure all combinations are tested.\n",
    "    for r in ranks:\n",
    "        for mi in maxIters:\n",
    "            for rp in regParams:\n",
    "                for a in alphas:\n",
    "                    # Create ALS model\n",
    "                    als = ALS(userCol=userCol, itemCol=itemCol, ratingCol=ratingCol,\n",
    "                              rank=r, maxIter=mi, regParam=rp, alpha=a,\n",
    "                              coldStartStrategy=\"drop\", nonnegative=True, implicitPrefs=True)\n",
    "\n",
    "                    # Fit model to each fold in the training set\n",
    "                    model1 = als.fit(train1)\n",
    "                    model2 = als.fit(train2)\n",
    "                    model3 = als.fit(train3)\n",
    "                    model4 = als.fit(train4)\n",
    "                    model5 = als.fit(train5)\n",
    "\n",
    "                    # Generating model's predictions for each fold in the test set\n",
    "                    predictions1 = model1.transform(test1)\n",
    "                    predictions2 = model2.transform(test2)\n",
    "                    predictions3 = model3.transform(test3)\n",
    "                    predictions4 = model4.transform(test4)\n",
    "                    predictions5 = model5.transform(test5)\n",
    "\n",
    "                    # Expected percentile rank error metric function\n",
    "                    def ROEM(predictions, userCol=userCol, itemCol=itemCol, ratingCol=ratingCol):\n",
    "                        # Creates table that can be queried\n",
    "                        predictions.createOrReplaceTempView(\"predictions_temp\")\n",
    "                        \n",
    "                        # Sum of total number of plays of all songs\n",
    "                        denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n",
    "                        \n",
    "                        # Calculating rankings of songs predictions by user\n",
    "                        spark.sql(f'''\n",
    "                            SELECT {userCol},\n",
    "                                   {ratingCol},\n",
    "                                   PERCENT_RANK() OVER (PARTITION BY {userCol} ORDER BY prediction DESC) AS rank\n",
    "                            FROM predictions_temp\n",
    "                        ''').createOrReplaceTempView(\"rankings_temp\")\n",
    "                        \n",
    "                        # Multiplies the rank of each song by the number of plays and adds the products together\n",
    "                        numerator = spark.sql(f'''\n",
    "                            SELECT SUM({ratingCol} * rank) FROM rankings_temp\n",
    "                        ''').collect()[0][0]\n",
    "                        performance = numerator/denominator\n",
    "                        return performance\n",
    "\n",
    "                    # Calculating expected percentile rank error metric for the model on each \n",
    "                    # fold's prediction set\n",
    "                    roem1 = ROEM(predictions1)\n",
    "                    roem2 = ROEM(predictions2)\n",
    "                    roem3 = ROEM(predictions3)\n",
    "                    roem4 = ROEM(predictions4)\n",
    "                    roem5 = ROEM(predictions5)\n",
    "                    \n",
    "                    #Validating the model's performance on the validation set\n",
    "                    validation_model = als.fit(train)\n",
    "                    validation_predictions = validation_model.transform(validate)\n",
    "                    validation_roem = ROEM(validation_predictions)\n",
    "                    \n",
    "                    # Printing the model's performance on each fold\n",
    "                    print(f'''\n",
    "                    Model Parameters:\n",
    "                                 Rank: {r}\n",
    "                              MaxIter: {mi}\n",
    "                             RegParam: {rp}\n",
    "                                Alpha: {a}\n",
    "                    Train Rank Errors: [{roem1:.3f}, {roem2:.3f}, {roem3:.3f}, {roem4:.3f}, {roem5:.3f}]\n",
    "                       Val Rank Error: {validation_roem}\n",
    "                    ''')\n",
    "\n",
    "                    # Filling in final hyperparameters with those of the best-performing model\n",
    "                    if validation_roem < best_validation_roem:\n",
    "                        best_validation_roem = validation_roem\n",
    "                        best_rank = r\n",
    "                        best_maxIter = mi\n",
    "                        best_regParam = rp\n",
    "                        best_alpha = a\n",
    "                        best_model = validation_model\n",
    "                        best_predictions = validation_predictions\n",
    "\n",
    "    # Printing best model's expected percentile rank and hyperparameters\n",
    "    print(f'''\n",
    "    Best Model\n",
    "    --------------------------------------\n",
    "    Rank Error: {best_validation_roem}\n",
    "          Rank: {best_rank}\n",
    "       MaxIter: {best_maxIter}\n",
    "      RegParam: {best_regParam}\n",
    "         Alpha: {best_alpha}\n",
    "    ''')\n",
    "    return best_model, best_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b5f956-7713-47fe-8f31-11399ce99d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 unique users.\n",
      "643 unique products.\n",
      "190328 total expected records.\n",
      "\n",
      "    Total models to create: 4\n",
      "    \n",
      "\n",
      "                    Model Parameters:\n",
      "                                 Rank: 10\n",
      "                              MaxIter: 10\n",
      "                             RegParam: 0.1\n",
      "                                Alpha: 5\n",
      "                    Train Rank Errors: [0.225, 0.199, 0.226, 0.138, 0.155]\n",
      "                       Val Rank Error: 0.10898280437892374\n",
      "                    \n",
      "\n",
      "                    Model Parameters:\n",
      "                                 Rank: 10\n",
      "                              MaxIter: 10\n",
      "                             RegParam: 0.1\n",
      "                                Alpha: 10\n",
      "                    Train Rank Errors: [0.140, 0.171, 0.147, 0.152, 0.186]\n",
      "                       Val Rank Error: 0.07936771917908436\n",
      "                    \n",
      "\n",
      "                    Model Parameters:\n",
      "                                 Rank: 20\n",
      "                              MaxIter: 10\n",
      "                             RegParam: 0.1\n",
      "                                Alpha: 5\n",
      "                    Train Rank Errors: [0.148, 0.175, 0.190, 0.214, 0.174]\n",
      "                       Val Rank Error: 0.11646676032698085\n",
      "                    \n",
      "\n",
      "                    Model Parameters:\n",
      "                                 Rank: 20\n",
      "                              MaxIter: 10\n",
      "                             RegParam: 0.1\n",
      "                                Alpha: 10\n",
      "                    Train Rank Errors: [0.175, 0.169, 0.183, 0.151, 0.140]\n",
      "                       Val Rank Error: 0.08972715121727262\n",
      "                    \n",
      "\n",
      "    Best Model\n",
      "    --------------------------------------\n",
      "    Rank Error: 0.07936771917908436\n",
      "          Rank: 10\n",
      "       MaxIter: 10\n",
      "      RegParam: 0.1\n",
      "         Alpha: 10\n",
      "    \n",
      "\n",
      "Best model: ALSModel: uid=ALS_acbb38cd200b, rank=10\n",
      "\n",
      "+------+------+--------+----------+\n",
      "|userId|songId|numPlays|prediction|\n",
      "+------+------+--------+----------+\n",
      "|22443 |3525  |50      |0.9907982 |\n",
      "|22975 |40311 |46      |0.98149806|\n",
      "|32331 |129142|32      |0.945254  |\n",
      "|85477 |354625|31      |0.9457757 |\n",
      "|99324 |349271|30      |0.9570207 |\n",
      "+------+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+------+--------+----------+\n",
      "|userId|songId|numPlays|prediction|\n",
      "+------+------+--------+----------+\n",
      "|110   |2242  |0       |0.0       |\n",
      "|110   |841   |0       |0.0       |\n",
      "|110   |71476 |0       |0.30359018|\n",
      "|110   |9119  |0       |0.0       |\n",
      "|110   |6449  |0       |0.0       |\n",
      "+------+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Consumed time: 6.365479973951976 min.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df = ratings_data.select('userId', 'songId', 'numPlays').repartition(5)\n",
    "df_zeros = add_zeros_to_ratings(df)\n",
    "\n",
    "best_model, best_predictions = ROEM_cv(ratings_df=df_zeros,\n",
    "                                       userCol='userId',\n",
    "                                       ranks=[10, 20],\n",
    "                                       maxIters=[10],\n",
    "                                       regParams=[.1,],\n",
    "                                       alphas=[5, 10],\n",
    "                                       seed=SEED)\n",
    "print(f'''\n",
    "Best model: {best_model}\n",
    "''')\n",
    "best_predictions.sort('numPlays', ascending=False).show(5, truncate=False)\n",
    "best_predictions.sort('numPlays').show(5, truncate=False)\n",
    "print(f'''\n",
    "Consumed time: {(time.time() - start)/60} min.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551b463-d3a5-43be-b505-009b6a23d8c5",
   "metadata": {},
   "source": [
    "# Binary Ratings using binary user behavior\n",
    "\n",
    "## With ROEM (Rank Ordering Error Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d08a9dc-ef69-43de-897c-b01c10f0a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|   178|    223|     1|2023-10-04 18:28:39|\n",
      "|   178|    260|     1|2023-10-04 18:36:52|\n",
      "|   178|    318|     1|2023-10-04 18:56:06|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the data\n",
    "binary_movies_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66b902d2-557f-4c38-8010-abe85ca09bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 unique users.\n",
      "728 unique products.\n",
      "232960 total expected records.\n",
      "\n",
      "    Total models to create: 2\n",
      "    \n",
      "\n",
      "                    Model Parameters:\n",
      "                                 Rank: 10\n",
      "                              MaxIter: 5\n",
      "                             RegParam: 0.1\n",
      "                                Alpha: 5\n",
      "                    Train Rank Errors: [0.270, 0.268, 0.268, 0.268, 0.270]\n",
      "                       Val Rank Error: 0.2598837376016439\n",
      "                    \n",
      "\n",
      "                    Model Parameters:\n",
      "                                 Rank: 10\n",
      "                              MaxIter: 5\n",
      "                             RegParam: 0.1\n",
      "                                Alpha: 10\n",
      "                    Train Rank Errors: [0.266, 0.261, 0.275, 0.263, 0.278]\n",
      "                       Val Rank Error: 0.25293775499993015\n",
      "                    \n",
      "\n",
      "    Best Model\n",
      "    --------------------------------------\n",
      "    Rank Error: 0.25293775499993015\n",
      "          Rank: 10\n",
      "       MaxIter: 5\n",
      "      RegParam: 0.1\n",
      "         Alpha: 10\n",
      "    \n",
      "\n",
      "Best model: ALSModel: uid=ALS_f240026bcc4b, rank=10\n",
      "\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|178   |10     |0     |0.34691322|\n",
      "|178   |104    |0     |0.4449077 |\n",
      "|178   |364    |0     |0.40777194|\n",
      "|178   |903    |0     |0.53358096|\n",
      "|178   |1219   |0     |0.5356273 |\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Consumed time: 9.502962060769399 min.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df = binary_movies_data.select('userId', 'movieId', 'rating').repartition(5)\n",
    "df_zeros = add_zeros_to_ratings(df, userCol='userId', productCol='movieId')\n",
    "\n",
    "best_model, best_predictions = ROEM_cv(ratings_df=df_zeros,\n",
    "                                       userCol='userId',\n",
    "                                       itemCol='movieId',\n",
    "                                       ratingCol='rating',\n",
    "                                       ranks=[10],\n",
    "                                       maxIters=[5],\n",
    "                                       regParams=[.1,],\n",
    "                                       alphas=[5, 10],\n",
    "                                       seed=SEED)\n",
    "print(f'''\n",
    "Best model: {best_model}\n",
    "''')\n",
    "best_predictions.show(5, truncate=False)\n",
    "print(f'''\n",
    "Consumed time: {(time.time() - start)/60} min.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1bfcef-d3b7-45e6-bee3-784106b2fa99",
   "metadata": {},
   "source": [
    "## Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "340bf745-6f8e-4201-83af-28594ac93f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
